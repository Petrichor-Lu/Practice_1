```{r setup, include=FALSE}

library(readr)          # Data Input
library(tidymodels)     # Data Manipulation
library(lubridate)      # Data Manupulation
library(dplyr)          # Data Manipulation
library(reshape2)       # Data Manipulation
library(caTools)        # Data Manipulation
library(corrplot)       # Data Visualisation
library(ggplot2)        # Data Visualization
library(viridis)        # Data Visualization
library(ggthemes)       # Data Visualization
library(pROC)           # Metrics
library(caret)          # Machine Learning
library(xgboost)        # xgboost model
```

This practical is based on exploratory data analysis and prediction of a dataset derived from a municipal database of healthcare administrative data. This dataset is derived from Vitoria, the capital city of Espírito Santo, Brazil (population 1.8 million) and was freely shared under a creative commons license.

**Generate an rmarkdown report that contains all the necessary code to document and perform: EDA, prediction of no-shows using XGBoost, and an analysis of variable/feature importance using this data set. Ensure your report includes answers to any questions marked in bold. Please submit your report via brightspace as a link to a git repository containing the rmarkdown and compiled/knitted html version of the notebook.**

## Introduction

The Brazilian public health system, known as SUS for Unified Health System in its acronym in Portuguese, is one of the largest health system in the world, representing government investment of more than 9% of GDP. However, its operation is not homogeneous and there are distinct perceptions of quality from citizens in different regions of the country.  Non-attendance of medical appointments contributes a significant additional burden on limited medical resources.  This analysis will try and investigate possible factors behind non-attendance using an administrative database of appointment data from Vitoria, Espírito Santo, Brazil.

### Understanding the data

****1** Use the data dictionary describe each of the variables/features in the CSV in your report.
**# Data Dictionary.  
#PatientID: Unique identifier for each patient.  
#AppointmentID: Unique identifier to each appointment.  
#Gender: Patient Gender (limited to Male or Female).  
#ScheduledDate: date on which the appointment was scheduled.  
#AppointmentDate: date of the actual appointment.  
#Age: Patient age.  
#Neighbourhood: District of Vitória in which the appointment.  
#SocialWelfare: Patient is a recipient of Bolsa Família welfare payments.  
#Hypertension: Patient previously diagnoised with hypertensio (Boolean).  
#Diabetes: Patient previously diagnosed with diabetes (Boolean).  
#AlcoholUseDisorder: Patient previously diagnosed with alcohol use disorder (Boolean).  
#Disability: Patient previously diagnosed with a disability (severity rated 0-4).  
#SMSReceived: At least 1 reminder text sent before appointment (Boolean).  
#NoShow: Patient did not attend scheduled appointment (Boolean: Yes/No).**

**2** Can you think of 3 hypotheses for why someone may be more likely to miss a medical appointment?
**1. They may remembered the wrong date. 
2. They may remembered the wrong address.
3. They may be tied with some personal things and fail to cancel the appointment.**

**3** Can you provide 3 examples of important contextual information that is missing in this data dictionary and dataset that could impact your analyses e.g., what type of medical appointment does each `AppointmentID` refer to? 
**#1.Length and weitht?  
#2.Initial or repeat visit?  
#3.Medical subjects appointed? **

## Data Parsing and Cleaning

**4** Modify the following to make it reproducible i.e., downloads the data file directly from version control
**coding as below**
```{r parse}
data <-  read_csv("/Users/terrylu/Desktop/Dal/Coureses/Summer/Applied_Res_in Health_Data_Sci/R_Tuto_Coding/week2/data.csv", col_type='fffTTifllllflf')
#data <- readr::read_csv('https://raw.githubusercontent.com/maguire-lab/health_data_science_research_2024/ ... ')
```

Now we need to check data is valid: because we specified col_types and the data parsed without error most of our data seems to at least be formatted as we expect i.e., ages are integers

```{r}
data %>% filter(Age > 110)
```
We can see there are 2 patient's older than 110 which seems suspicious but we can't actually say if this is impossible.

**5** Are there any individuals with impossible ages? If so we can drop this row using `filter` i.e., `data <- data %>% filter(CRITERIA)`
**Coding as below**
```{r}
data <- data %>% dplyr::filter(Age <= 110)
```

## Exploratory Data Analysis
First, we should get an idea if the data meets our expectations, there are newborns in the data (`Age==0`) and we wouldn't expect any of these to be diagnosed with Diabetes, Alcohol Use Disorder, and Hypertension (although in theory it could be possible).  We can easily check this:

```{r}
data %>% filter(Age == 0) %>% select(Hypertension, Diabetes, AlcoholUseDisorder) %>% unique()
```

We can also explore things like how many different neighborhoods are there and how many appoints are from each? 

```{r}
count(data, Neighbourhood, sort = TRUE)
```
#sort = TRUE: ranking form big to small


**6** What is the maximum number of appointments from the same patient?
**#88 as shown below**

```{r}
count(data, PatientID, sort = TRUE)
```


Let's explore the correlation between variables:

```{r}

# let's define a plotting function
corplot = function(df){
  
  cor_matrix_raw <- round(cor(df),2)
  cor_matrix <- melt(cor_matrix_raw)
  
  
  #Get triangle of the correlation matrix
  #Lower Triangle
  get_lower_tri<-function(cor_matrix_raw){
    cor_matrix_raw[upper.tri(cor_matrix_raw)] <- NA
    return(cor_matrix_raw)
  }
  
  # Upper Triangle
  get_upper_tri <- function(cor_matrix_raw){
    cor_matrix_raw[lower.tri(cor_matrix_raw)]<- NA
    return(cor_matrix_raw)
  }
  
  upper_tri <- get_upper_tri(cor_matrix_raw)
  
  # Melt the correlation matrix
  cor_matrix <- melt(upper_tri, na.rm = TRUE)
  
  # Heatmap Plot
  cor_graph <- ggplot(data = cor_matrix, aes(Var2, Var1, fill = value))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "darkorchid", high = "orangered", mid = "grey50", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Pearson\nCorrelation") +
    theme_minimal()+ 
    theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                     size = 8, hjust = 1))+
    coord_fixed()+ geom_text(aes(Var2, Var1, label = value), color = "black", size = 2) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank())+
      ggtitle("Correlation Heatmap")+
      theme(plot.title = element_text(hjust = 0.5))
  
  cor_graph
}

numeric.data = mutate_all(data, function(x) as.numeric(x))

# Plot Correlation Heatmap
corplot(numeric.data)

```

Correlation heatmaps are useful for identifying linear relationships between variables/features.
In this case, we are particularly interested in relationships between `NoShow` and any specific variables.

**7** Which parameters most strongly correlate with missing appointments (`NoShow`)?
**It's very clear that "ScheduleDate" shows the most strongly correlate with missing appointments, with the coefficent of "-0.16" **

**8** Are there any other variables which strongly correlate with one another?
**Yes, the second variable is "SMSReceived", showing the coefficent of "0.13".**

"correlation" doesn't mean "significance" here. Before doing the test, we couldn't say significant or not, even if it's clear.

**9** Do you see any issues with PatientID/AppointmentID being included in this plot? 
**PatientID and AppointmentID are records of identities. Since it's totully random, so they do not show any meaningful correlation with other variables.**

Let's look at some individual variables and their relationship with `NoShow`.

```{r,fig.align="center"}
ggplot(data) + 
  geom_density(aes(x=Age, fill=NoShow), alpha=0.8) + 
  ggtitle("Density of Age by Attendence")
```
There does seem to be a difference in the distribution of ages of people that miss and don't miss appointments.  
However, the shape of this distribution means the actual correlation is near 0 in the heatmap above. This highlights the need to look at individual variables.

Let's take a closer look at age by breaking it into categories.

```{r, fig.align="center"}
data <- data %>% mutate(Age.Range=cut_interval(Age, length=10))

ggplot(data) + 
  geom_bar(aes(x=Age.Range, fill=NoShow)) + 
  ggtitle("Amount of No Show across Age Ranges")
```

```{r,fig.align="center"}
ggplot(data) + 
  geom_bar(aes(x=Age.Range, fill=NoShow), position='fill') + 
  ggtitle("Proportion of No Show across Age Ranges")

```

**10** How could you be misled if you only plotted 1 of these 2 plots of attendance by age group?
**The first plot shows the frequency distribution, while the second plot shows the relative proportions distribution.
If I only look at the first plot, I might mistakenly believe that people aged (0, 60] are more likely to miss appointments. 
If I only look at the second plot, I might mistakenly believe that people aged (100, 110] are more likely to miss appointments, but this plot does not inform me about the small sample size.
The key takeaway from this is that  number of individuals > 90 are very few from plot 1 so probably are very small so unlikely to make much of an impact on the overall distributions. However, other patterns do emerge such as 10-20 age group is nearly twice as likely to miss appointments as the 60-70 years old.**

Next, we'll have a look at `SMSReceived` variable:

```{r,fig.align="center"}
ggplot(data) + 
  geom_bar(aes(x=SMSReceived, fill=NoShow), alpha=0.8) + 
  ggtitle("Attendance by SMS Received")
```

```{r,fig.align="center"}
ggplot(data) + 
  geom_bar(aes(x=SMSReceived, fill=NoShow), position='fill', alpha=0.8) + 
  ggtitle("Proportion Attendance by SMS Received")
```


**11** From this plot does it look like SMS reminders increase or decrease the chance of someone not attending an appointment? Why might the opposite actually be true (hint: think about biases)?
**From the charts, it appears that receiving SMS reminders seems to increase the likelihood of missing an appointment. However, this might be due to biases. For example, the appointment interface might have an option asking "Do you want to receive SMS reminders?" Self-disciplined individuals who tend to attend appointments on time might refuse this service because they don't need it. In contrast, less disciplined individuals who tend to miss appointments might opt for this service. Therefore, the sample of data for those who choose SMS reminders might already include many individuals who are inclined to miss appointments.**


**12** Create a similar plot which compares the the density of `NoShow` across the values of disability 
**Coding as shown below**
```{r} 
ggplot(data) + 
  geom_density(aes(x=Disability, fill=NoShow), alpha=0.8) + 
  ggtitle("Density of Disbility by Attendence")
```


Now let's look at the neighbourhood data as location can correlate highly with many social determinants of health. 

```{r, fig.align="center"}
ggplot(data) + 
  geom_bar(aes(x=Neighbourhood, fill=NoShow)) + 
  theme(axis.text.x = element_text(angle=45, hjust=1, size=5)) + 
  ggtitle('Attendance by Neighbourhood')
```

```{r, fig.align="center"}
ggplot(data) + 
  geom_bar(aes(x=Neighbourhood, fill=NoShow), position='fill') + 
  theme(axis.text.x = element_text(angle=45, hjust=1, size=5)) + 
  ggtitle('Proportional Attendance by Neighbourhood')
```

Most neighborhoods have similar proportions of no-show but some have much higher and lower rates.

**13** Suggest a reason for differences in attendance rates across neighbourhoods.
The neighborhoods showing much higher or lower attendance rates have relatively fewer cases/samples. Therefore, small differences in the number of missed or attended appointments can exaggerate the proportions.

Now let's explore the relationship between gender and NoShow.
**Coding as shown as below**
```{r, fig.align="center"}
ggplot(data) + 
  geom_bar(aes(x=Gender, fill=NoShow))+
  ggtitle("Gender by attendance")
```

```{r, fig.align="center"}
ggplot(data) + 
  geom_bar(aes(x=Gender, fill=NoShow), position='fill')+
  ggtitle("Proportion Gender by attendance")
```

**14** Create a similar plot using `SocialWelfare`
**Coding as shown as below**
```{r ,fig.align="center"}
ggplot(data) + 
  geom_bar(aes(x=SocialWelfare, fill=NoShow))+
  ggtitle("Gender by attendance")
```
```{r ,fig.align="center"}
ggplot(data) + 
  geom_bar(aes(x=SocialWelfare, fill=NoShow),position="fill")+
  ggtitle("Gender by attendance")
```

Far more exploration could still be done, including dimensionality reduction approaches but although we have found some patterns there is no major/striking patterns on the data as it currently stands.

However, maybe we can generate some new features/variables that more strongly relate to the `NoShow`.

## Feature Engineering

Let's begin by seeing if appointments on any day of the week has more no-show's. Fortunately, the `lubridate` library makes this quite easy!

```{r}
data <- data %>% mutate(AppointmentDay = wday(AppointmentDate, label=TRUE, abbr=TRUE), 
                                 ScheduledDay = wday(ScheduledDate,  label=TRUE, abbr=TRUE))

ggplot(data) +
  geom_bar(aes(x=AppointmentDay, fill=NoShow)) +
  ggtitle("Amount of No Show across Appointment Day") 
```

```{r}
ggplot(data) +
  geom_bar(aes(x=AppointmentDay, fill=NoShow), position = 'fill') +
  ggtitle("Proportion of No Show across Appointment Day") 

```
Let's begin by creating a variable called `Lag`, which is the difference between when an appointment was scheduled and the actual appointment.

```{r, fig.align="center"}
data <- data %>% mutate(Lag.days=difftime(AppointmentDate, ScheduledDate, units = "days"),
                                Lag.hours=difftime(AppointmentDate, ScheduledDate, units = "hours"))

ggplot(data) + 
  geom_density(aes(x=Lag.days, fill=NoShow), alpha=0.7)+
  ggtitle("Density of Lag (days) by attendance")
```

**15** Have a look at the values in lag variable, does anything seem odd?
**A large number of appointments are scheduled close to the visit date. Most of these appointments are attended, while appointments scheduled farther from the actual appointment date tend to have a higher proportion of no-shows.
Odd:
The occurrence of negative values in the lag data is very odd, as it means that the actual appointment date is earlier than the appointment-scheduling date. This is unreasonable.**

## Predictive Modeling

Let's see how well we can predict NoShow from the data. 

We'll start by preparing the data, followed by splitting it into testing and training set, modeling and finally, evaluating our results. For now we will subsample but please run on full dataset for final execution.


```{r}
### REMOVE SUBSAMPLING FOR FINAL MODEL，#%>% sample_n(10000)" => select 10000 samples randomly
data.prep <- data %>% select(-AppointmentID, -PatientID) #%>% sample_n(10000)

#set random seed
set.seed(42)

#split the data to training data and testing data
data.split <- initial_split(data.prep, prop = 0.7)

#set up set
train  <- training(data.split)
test <- testing(data.split)
```

Let's now set the cross validation parameters, and add classProbs so we can use AUC as a metric for xgboost.

```{r}
fit.control <- trainControl(method="cv",number=3,
                           classProbs = TRUE, summaryFunction = twoClassSummary)
 # use k-fold cross-validation, 3 Fold-over vaerfication,Enable category probability calculation,use twoClassSummary Calculation and evaluation Indicator.
```

**16** Based on the EDA, how well do you think this is going to work?
**Based on the EDA, I believe the prediction can be successful, but it might be inaccurate for certain categories, such as higher age groups. This is because there is a lack of samples in these extreme areas, which can affect accurate modeling and prediction.**

Now we can train our XGBoost model
```{r}
xgb.grid <- expand.grid(eta=c(0.05),
                       max_depth=c(4),colsample_bytree=1,
                       subsample=1, nrounds=500, gamma=0, min_child_weight=5)

xgb.model <- train(NoShow ~ .,data=train, method="xgbTree",metric="ROC",
                  tuneGrid=xgb.grid, trControl=fit.control)

xgb.pred <- predict(xgb.model, newdata=test)
xgb.probs <- predict(xgb.model, newdata=test, type="prob")
```

```{r}
test <- test %>% mutate(NoShow.numerical = ifelse(NoShow=="Yes",1,0))
confusionMatrix(xgb.pred, test$NoShow, positive="Yes")
paste("XGBoost Area under ROC Curve: ", round(auc(test$NoShow.numerical, xgb.probs[,2]),3), sep="")
```

This isn't an unreasonable performance, but let's look a bit more carefully at the correct and incorrect predictions,

```{r ,fig.align="center"}
xgb.probs$Actual = test$NoShow.numerical
xgb.probs$ActualClass = test$NoShow
xgb.probs$PredictedClass = xgb.pred
xgb.probs$Match = ifelse(xgb.probs$ActualClass == xgb.probs$PredictedClass,
                         "Correct","Incorrect")
# [4.8] Plot Accuracy
xgb.probs$Match = factor(xgb.probs$Match,levels=c("Incorrect","Correct"))
ggplot(xgb.probs,aes(x=Yes,y=Actual,color=Match))+
  geom_jitter(alpha=0.2,size=0.25)+
  scale_color_manual(values=c("grey40","orangered"))+
  ggtitle("Visualizing Model Performance", "(Dust Plot)")
```


Finally, let's close it off with the variable importance of our model:

```{r,fig.align="center"}
results = data.frame(Feature = rownames(varImp(xgb.model)$importance)[1:10],
                     Importance = varImp(xgb.model)$importance[1:10,])

results$Feature = factor(results$Feature,levels=results$Feature)


# [4.10] Plot Variable Importance
ggplot(results, aes(x=Feature, y=Importance,fill=Importance))+
  geom_bar(stat="identity")+
  scale_fill_gradient(low="grey20",high="orangered")+
  ggtitle("XGBoost Variable Importance")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**17** Using the [caret package](https://topepo.github.io/caret/) fit and evaluate 1 other ML model on this data.
**Coding as shown below**
```{r}
library(caret)
set.seed(42)
data.prep <- data %>% select(-AppointmentID, -PatientID)
data.split <- initial_split(data.prep, prop = 0.7)
train <- training(data.split)
test <- testing(data.split)

# set the parameter of the training
fit.control <- trainControl(method = "cv", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)

# training
logistic.model <- train(NoShow ~ ., data = train, method = "glm", family = "binomial", metric = "ROC", trControl = fit.control)

# evalue
logistic.pred <- predict(logistic.model, newdata = test)
logistic.probs <- predict(logistic.model, newdata = test, type = "prob")

test <- test %>% mutate(NoShow.numerical = ifelse(NoShow == "Yes", 1, 0))
confusionMatrix(logistic.pred, test$NoShow, positive = "Yes")
roc_curve <- roc(test$NoShow.numerical, logistic.probs[, 2])
auc_value <- auc(roc_curve)
paste("Logistic Regression Area under ROC Curve: ", round(auc_value, 3))
```


```{r}
logistic.probs$Actual <- test$NoShow.numerical
logistic.probs$ActualClass <- test$NoShow
logistic.probs$PredictedClass <- logistic.pred
logistic.probs$Match <- ifelse(logistic.probs$ActualClass == logistic.probs$PredictedClass, "Correct", "Incorrect")
logistic.probs$Match <- factor(logistic.probs$Match, levels = c("Incorrect", "Correct"))

ggplot(logistic.probs, aes(x = Yes, y = Actual, color = Match)) +
  geom_jitter(alpha = 0.2, size = 0.25) +
  scale_color_manual(values = c("grey40", "orangered")) +
  ggtitle("Visualizing Model Performance", "(Dust Plot)")
```


```{r}
# Extract variable importance using varImp
var_imp <- varImp(logistic.model)

# Convert variable importance to a data frame
results = data.frame(Feature = rownames(var_imp$importance),
                     Importance = var_imp$importance[,1])

# Set factor levels
results$Feature = factor(results$Feature, levels = results$Feature)

# Plot 
ggplot(results, aes(x = Feature, y = Importance, fill = Importance)) +
  geom_bar(stat = "identity") +
  scale_fill_gradient(low = "grey20", high = "orangered") +
  ggtitle("Logistic Regression Variable Importance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```




**18** Based on everything, do you think we can trust analyses based on this dataset? Explain your reasoning.

**I don't think we can fully trust analyses based on this dataset. Although the accuracy of the two models are 80.18% (XGBoost) and 79.66% (logistic regression),high and close to each other. However, the sensitivity of the first model (XGBoost) is 3.1%, and for the second model (logistic regression), it's even lower at 1.8%. Both sensitivities are very low.**

## Credits

This notebook was based on a combination of other notebooks e.g., [1](https://www.kaggle.com/code/tsilveira/applying-heatmaps-for-categorical-data-analysis), [2](https://www.kaggle.com/code/samratp/predict-show-noshow-eda-visualization-model), [3](https://www.kaggle.com/code/andrewmvd/exploring-and-predicting-no-shows-with-xgboost/report)